{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 34 samples with A549 cell line:\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pyarrow.dataset as ds\n",
    "import gcsfs\n",
    "import os\n",
    "import subprocess\n",
    "\n",
    "# Initialize GCS file system\n",
    "fs = gcsfs.GCSFileSystem()\n",
    "\n",
    "# GCS base path\n",
    "gcs_base_path = \"gs://arc-ctc-scbasecamp/2025-02-25/\"\n",
    "\n",
    "# STARsolo feature type\n",
    "feature_type = \"GeneFull_Ex50pAS\"\n",
    "\n",
    "# Metadata path\n",
    "metadata_path = os.path.join(gcs_base_path, \"metadata\", feature_type)\n",
    "\n",
    "# Get the sample metadata file path for Homo sapiens\n",
    "sample_metadata_path = os.path.join(metadata_path, \"Homo_sapiens\", \"sample_metadata.parquet\")\n",
    "\n",
    "# Load the metadata\n",
    "sample_metadata = ds.dataset(sample_metadata_path, filesystem=fs, format=\"parquet\").to_table().to_pandas()\n",
    "\n",
    "# Filter for A549 cell line\n",
    "a549_samples = sample_metadata[sample_metadata[\"cell_line\"].str.contains(\"A549\", na=False)]\n",
    "\n",
    "# Print the results\n",
    "print(f\"Found {len(a549_samples)} samples with A549 cell line:\")\n",
    "# print(a549_samples[[\"srx_accession\", \"tissue\", \"disease\", \"perturbation\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading gs://arc-ctc-scbasecamp/2025-02-25/h5ad/GeneFull_Ex50pAS/Homo_sapiens/ERX8792190.h5ad to a549_data/ERX8792190.h5ad...\n",
      "Downloading gs://arc-ctc-scbasecamp/2025-02-25/h5ad/GeneFull_Ex50pAS/Homo_sapiens/SRX21897869.h5ad to a549_data/SRX21897869.h5ad...\n",
      "Downloading gs://arc-ctc-scbasecamp/2025-02-25/h5ad/GeneFull_Ex50pAS/Homo_sapiens/SRX25289894.h5ad to a549_data/SRX25289894.h5ad...\n",
      "Downloading gs://arc-ctc-scbasecamp/2025-02-25/h5ad/GeneFull_Ex50pAS/Homo_sapiens/SRX17488180.h5ad to a549_data/SRX17488180.h5ad...\n",
      "Downloading gs://arc-ctc-scbasecamp/2025-02-25/h5ad/GeneFull_Ex50pAS/Homo_sapiens/SRX17915869.h5ad to a549_data/SRX17915869.h5ad...\n",
      "Downloading gs://arc-ctc-scbasecamp/2025-02-25/h5ad/GeneFull_Ex50pAS/Homo_sapiens/SRX19004457.h5ad to a549_data/SRX19004457.h5ad...\n",
      "Downloading gs://arc-ctc-scbasecamp/2025-02-25/h5ad/GeneFull_Ex50pAS/Homo_sapiens/SRX24227811.h5ad to a549_data/SRX24227811.h5ad...\n",
      "Downloading gs://arc-ctc-scbasecamp/2025-02-25/h5ad/GeneFull_Ex50pAS/Homo_sapiens/SRX25289882.h5ad to a549_data/SRX25289882.h5ad...\n",
      "Downloading gs://arc-ctc-scbasecamp/2025-02-25/h5ad/GeneFull_Ex50pAS/Homo_sapiens/SRX26771412.h5ad to a549_data/SRX26771412.h5ad...\n",
      "Downloading gs://arc-ctc-scbasecamp/2025-02-25/h5ad/GeneFull_Ex50pAS/Homo_sapiens/SRX17150748.h5ad to a549_data/SRX17150748.h5ad...\n",
      "Downloading gs://arc-ctc-scbasecamp/2025-02-25/h5ad/GeneFull_Ex50pAS/Homo_sapiens/SRX17150749.h5ad to a549_data/SRX17150749.h5ad...\n",
      "Downloading gs://arc-ctc-scbasecamp/2025-02-25/h5ad/GeneFull_Ex50pAS/Homo_sapiens/SRX17150750.h5ad to a549_data/SRX17150750.h5ad...\n",
      "Downloading gs://arc-ctc-scbasecamp/2025-02-25/h5ad/GeneFull_Ex50pAS/Homo_sapiens/SRX17150747.h5ad to a549_data/SRX17150747.h5ad...\n",
      "Downloading gs://arc-ctc-scbasecamp/2025-02-25/h5ad/GeneFull_Ex50pAS/Homo_sapiens/SRX17915870.h5ad to a549_data/SRX17915870.h5ad...\n",
      "Downloading gs://arc-ctc-scbasecamp/2025-02-25/h5ad/GeneFull_Ex50pAS/Homo_sapiens/SRX17941758.h5ad to a549_data/SRX17941758.h5ad...\n",
      "Downloading gs://arc-ctc-scbasecamp/2025-02-25/h5ad/GeneFull_Ex50pAS/Homo_sapiens/SRX17941757.h5ad to a549_data/SRX17941757.h5ad...\n",
      "Downloading gs://arc-ctc-scbasecamp/2025-02-25/h5ad/GeneFull_Ex50pAS/Homo_sapiens/SRX19215444.h5ad to a549_data/SRX19215444.h5ad...\n",
      "Downloading gs://arc-ctc-scbasecamp/2025-02-25/h5ad/GeneFull_Ex50pAS/Homo_sapiens/SRX19215443.h5ad to a549_data/SRX19215443.h5ad...\n",
      "Downloading gs://arc-ctc-scbasecamp/2025-02-25/h5ad/GeneFull_Ex50pAS/Homo_sapiens/SRX21897873.h5ad to a549_data/SRX21897873.h5ad...\n",
      "Downloading gs://arc-ctc-scbasecamp/2025-02-25/h5ad/GeneFull_Ex50pAS/Homo_sapiens/SRX21897872.h5ad to a549_data/SRX21897872.h5ad...\n",
      "Downloading gs://arc-ctc-scbasecamp/2025-02-25/h5ad/GeneFull_Ex50pAS/Homo_sapiens/SRX22159982.h5ad to a549_data/SRX22159982.h5ad...\n",
      "Downloading gs://arc-ctc-scbasecamp/2025-02-25/h5ad/GeneFull_Ex50pAS/Homo_sapiens/SRX25289881.h5ad to a549_data/SRX25289881.h5ad...\n",
      "Downloading gs://arc-ctc-scbasecamp/2025-02-25/h5ad/GeneFull_Ex50pAS/Homo_sapiens/SRX25289889.h5ad to a549_data/SRX25289889.h5ad...\n",
      "Downloading gs://arc-ctc-scbasecamp/2025-02-25/h5ad/GeneFull_Ex50pAS/Homo_sapiens/SRX25289885.h5ad to a549_data/SRX25289885.h5ad...\n",
      "Downloading gs://arc-ctc-scbasecamp/2025-02-25/h5ad/GeneFull_Ex50pAS/Homo_sapiens/SRX25289888.h5ad to a549_data/SRX25289888.h5ad...\n",
      "Downloading gs://arc-ctc-scbasecamp/2025-02-25/h5ad/GeneFull_Ex50pAS/Homo_sapiens/SRX25289887.h5ad to a549_data/SRX25289887.h5ad...\n",
      "Downloading gs://arc-ctc-scbasecamp/2025-02-25/h5ad/GeneFull_Ex50pAS/Homo_sapiens/SRX25289879.h5ad to a549_data/SRX25289879.h5ad...\n",
      "Downloading gs://arc-ctc-scbasecamp/2025-02-25/h5ad/GeneFull_Ex50pAS/Homo_sapiens/SRX25289890.h5ad to a549_data/SRX25289890.h5ad...\n",
      "Downloading gs://arc-ctc-scbasecamp/2025-02-25/h5ad/GeneFull_Ex50pAS/Homo_sapiens/SRX25289886.h5ad to a549_data/SRX25289886.h5ad...\n",
      "Downloading gs://arc-ctc-scbasecamp/2025-02-25/h5ad/GeneFull_Ex50pAS/Homo_sapiens/SRX25289891.h5ad to a549_data/SRX25289891.h5ad...\n",
      "Downloading gs://arc-ctc-scbasecamp/2025-02-25/h5ad/GeneFull_Ex50pAS/Homo_sapiens/SRX25289884.h5ad to a549_data/SRX25289884.h5ad...\n",
      "Downloading gs://arc-ctc-scbasecamp/2025-02-25/h5ad/GeneFull_Ex50pAS/Homo_sapiens/SRX25289880.h5ad to a549_data/SRX25289880.h5ad...\n",
      "Downloading gs://arc-ctc-scbasecamp/2025-02-25/h5ad/GeneFull_Ex50pAS/Homo_sapiens/SRX25289893.h5ad to a549_data/SRX25289893.h5ad...\n",
      "Downloading gs://arc-ctc-scbasecamp/2025-02-25/h5ad/GeneFull_Ex50pAS/Homo_sapiens/SRX25289892.h5ad to a549_data/SRX25289892.h5ad...\n",
      "All A549 h5ad files downloaded to a549_data\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Create a directory to save the files\n",
    "output_dir = \"a549_data\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Download the files using gsutil\n",
    "for i, row in a549_samples.iterrows():\n",
    "    file_path = row[\"file_path\"]\n",
    "    output_file = os.path.join(output_dir, os.path.basename(file_path))\n",
    "    \n",
    "    print(f\"Downloading {file_path} to {output_file}...\")\n",
    "    \n",
    "    # Using subprocess to call gsutil\n",
    "    fs.get(file_path, output_file)\n",
    "\n",
    "print(f\"All A549 h5ad files downloaded to {output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 34 A549 h5ad files\n",
      "Loading SRX25289881.h5ad...\n",
      "Loading SRX19215443.h5ad...\n",
      "Loading SRX25289889.h5ad...\n",
      "Loading SRX21897872.h5ad...\n",
      "Loading SRX25289894.h5ad...\n",
      "Loading SRX25289888.h5ad...\n",
      "Loading SRX17150747.h5ad...\n",
      "Loading SRX17915869.h5ad...\n",
      "Loading SRX21897873.h5ad...\n",
      "Loading SRX17150748.h5ad...\n",
      "Loading SRX17941757.h5ad...\n",
      "Loading SRX26771412.h5ad...\n",
      "Loading SRX17941758.h5ad...\n",
      "Loading ERX8792190.h5ad...\n",
      "Loading SRX25289892.h5ad...\n",
      "Loading SRX24227811.h5ad...\n",
      "Loading SRX25289893.h5ad...\n",
      "Loading SRX25289890.h5ad...\n",
      "Loading SRX25289884.h5ad...\n",
      "Loading SRX17488180.h5ad...\n",
      "Loading SRX22159982.h5ad...\n",
      "Loading SRX21897869.h5ad...\n",
      "Loading SRX17915870.h5ad...\n",
      "Loading SRX25289887.h5ad...\n",
      "Loading SRX17150750.h5ad...\n",
      "Loading SRX19004457.h5ad...\n",
      "Loading SRX25289886.h5ad...\n",
      "Loading SRX25289891.h5ad...\n",
      "Loading SRX25289880.h5ad...\n",
      "Loading SRX17150749.h5ad...\n",
      "Loading SRX25289879.h5ad...\n",
      "Loading SRX25289885.h5ad...\n",
      "Loading SRX19215444.h5ad...\n",
      "Loading SRX25289882.h5ad...\n",
      "Combining data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_718367/473817477.py:40: FutureWarning: Use anndata.concat instead of AnnData.concatenate, AnnData.concatenate is deprecated and will be removed in the future. See the tutorial for concat at: https://anndata.readthedocs.io/en/latest/concatenation.html\n",
      "  combined = adatas[0].concatenate(adatas[1:], join='outer', index_unique='-')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined data shape: (252338, 36601)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scanpy as sc\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "import anndata\n",
    "\n",
    "# Directory containing the downloaded A549 files\n",
    "data_dir = \"a549_data\"\n",
    "\n",
    "# List all h5ad files in the directory\n",
    "a549_files = [file for file in os.listdir(data_dir) if file.endswith('.h5ad')]\n",
    "print(f\"Found {len(a549_files)} A549 h5ad files\")\n",
    "\n",
    "# Create output directories\n",
    "results_dir = \"a549_perturbation_analysis\"\n",
    "os.makedirs(results_dir, exist_ok=True)\n",
    "fig_dir = os.path.join(results_dir, \"figures\")\n",
    "os.makedirs(fig_dir, exist_ok=True)\n",
    "\n",
    "# Load all datasets\n",
    "adatas = []\n",
    "for file in a549_files:\n",
    "    file_path = os.path.join(data_dir, file)\n",
    "    print(f\"Loading {file}...\")\n",
    "    try:\n",
    "        adata = sc.read_h5ad(file_path)\n",
    "        # Extract accession ID from filename\n",
    "        accession = file.replace('.h5ad', '')\n",
    "        adata.obs['sample_id'] = accession\n",
    "        adatas.append(adata)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {file}: {e}\")\n",
    "\n",
    "\n",
    "# Combine the data\n",
    "print(\"Combining data...\")\n",
    "combined = adatas[0].concatenate(adatas[1:], join='outer', index_unique='-')\n",
    "print(f\"Combined data shape: {combined.shape}\")\n",
    "\n",
    "# Basic preprocessing\n",
    "sc.pp.filter_cells(combined, min_genes=200)\n",
    "sc.pp.filter_genes(combined, min_cells=10)\n",
    "# sc.pp.normalize_total(combined, target_sum=1e4)\n",
    "# sc.pp.log1p(combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow.dataset as ds\n",
    "import gcsfs\n",
    "\n",
    "fs = gcsfs.GCSFileSystem()\n",
    "gcs_base_path = \"gs://arc-ctc-scbasecamp/2025-02-25/\"\n",
    "feature_type = \"GeneFull_Ex50pAS\"\n",
    "metadata_path = os.path.join(gcs_base_path, \"metadata\", feature_type)\n",
    "sample_metadata_path = os.path.join(metadata_path, \"Homo_sapiens\", \"sample_metadata.parquet\")\n",
    "sample_metadata = ds.dataset(sample_metadata_path, filesystem=fs, format=\"parquet\").to_table().to_pandas()\n",
    "\n",
    "# Filter for A549 cell line and extract perturbation information\n",
    "a549_metadata = sample_metadata[sample_metadata[\"cell_line\"].str.contains(\"A549\", na=False)]\n",
    "\n",
    "# Map sample_id to perturbation information\n",
    "id_to_perturbation = dict(zip(a549_metadata[\"srx_accession\"], a549_metadata[\"perturbation\"]))\n",
    "\n",
    "# Add perturbation information to the anndata object\n",
    "combined.obs['perturbation'] = combined.obs['sample_id'].map(id_to_perturbation).fillna('unknown')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing differential expression for 28 drugs. Drug list: ['Infected with H1N1 (A/California/07/2009), 8 hours post infection'\n",
      " 'glyconanomaterials for combating bacterial infections'\n",
      " 'infected with H3N2 (A/Perth/16/2009)'\n",
      " 'CAR T cell therapy, SUV39H1 knockout' 'uninfected (mock treatment)'\n",
      " 'irradiation' 'NS1 4xstop (mutant)'\n",
      " 'CAR T cell therapy with SUV39H1 knockout'\n",
      " 'irradiated A549 cells (6 Gy γ-ray treatment)'\n",
      " 'lentiviral pool for expression of 120 gRNAs, tamoxifen, puromycin'\n",
      " 'mixed sample treatments include DMSO, ARS-1620, and Vemurafenib'\n",
      " 'Ritonavir, gemcitabine, cisplatin'\n",
      " 'Infected with H3N2 (A/Perth/16/2009), 16 hours post infection'\n",
      " 'ACME HS dissociation'\n",
      " 'infected with H3N2, Cetuximab (anti-EGFR therapy)'\n",
      " 'Infected (Cal07, 16 hours, Replicate 2)' 'in vitro culture of iPSC-RPE'\n",
      " 'antineoplastic agents, mixed micelles for drug delivery'\n",
      " 'CAR T cell therapy, SUV39H1 knockout, CD19 tumor cell treatment'\n",
      " 'wild-type (WT) virus'\n",
      " 'infected with H3N2 (A/Perth/16/2009), 16 hours post infection'\n",
      " 'irradiation (IR 6h)' 'Bexmarilimab' 'uninfected (Mock)'\n",
      " 'infected with H1N1 (A/California/07/2009)' '6 Gy γ-ray treatment'\n",
      " 'unsure' '8 hours post infection']\n"
     ]
    }
   ],
   "source": [
    "unique_drugs = combined.obs['perturbation'].unique()\n",
    "# unique_drugs = [drug for drug in unique_drugs if drug != 'control']\n",
    "\n",
    "print(f\"Analyzing differential expression for {len(unique_drugs)} drugs. Drug list: {unique_drugs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing differential expression for 27 drugs. Drug list: ['Infected with H1N1 (A/California/07/2009), 8 hours post infection'\n",
      " 'glyconanomaterials for combating bacterial infections'\n",
      " 'infected with H3N2 (A/Perth/16/2009)'\n",
      " 'CAR T cell therapy, SUV39H1 knockout' 'control' 'irradiation'\n",
      " 'NS1 4xstop (mutant)' 'CAR T cell therapy with SUV39H1 knockout'\n",
      " 'irradiated A549 cells (6 Gy γ-ray treatment)'\n",
      " 'lentiviral pool for expression of 120 gRNAs, tamoxifen, puromycin'\n",
      " 'mixed sample treatments include DMSO, ARS-1620, and Vemurafenib'\n",
      " 'Ritonavir, gemcitabine, cisplatin'\n",
      " 'Infected with H3N2 (A/Perth/16/2009), 16 hours post infection'\n",
      " 'ACME HS dissociation'\n",
      " 'infected with H3N2, Cetuximab (anti-EGFR therapy)'\n",
      " 'Infected (Cal07, 16 hours, Replicate 2)' 'in vitro culture of iPSC-RPE'\n",
      " 'antineoplastic agents, mixed micelles for drug delivery'\n",
      " 'CAR T cell therapy, SUV39H1 knockout, CD19 tumor cell treatment'\n",
      " 'wild-type (WT) virus'\n",
      " 'infected with H3N2 (A/Perth/16/2009), 16 hours post infection'\n",
      " 'irradiation (IR 6h)' 'Bexmarilimab'\n",
      " 'infected with H1N1 (A/California/07/2009)' '6 Gy γ-ray treatment'\n",
      " 'unsure' '8 hours post infection']。 length: 27\n"
     ]
    }
   ],
   "source": [
    "# 假设 combined 是你的 AnnData 对象\n",
    "# 将 'uninfected (mock treatment)' 和 'uninfected (Mock)' 替换为 'control'\n",
    "combined.obs['perturbation'] = combined.obs['perturbation'].replace(\n",
    "    ['uninfected (mock treatment)', 'uninfected (Mock)'], 'control'\n",
    ")\n",
    "\n",
    "# 验证修改后的唯一值\n",
    "unique_drugs = combined.obs['perturbation'].unique()\n",
    "print(f\"Analyzing differential expression for {len(unique_drugs)} drugs. Drug list: {unique_drugs}。 length: {len(unique_drugs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "perturbation\n",
      "ACME HS dissociation                                                 38909\n",
      "irradiation                                                          17272\n",
      "irradiation (IR 6h)                                                  16167\n",
      "infected with H3N2 (A/Perth/16/2009)                                 15949\n",
      "6 Gy γ-ray treatment                                                 15126\n",
      "CAR T cell therapy, SUV39H1 knockout, CD19 tumor cell treatment      13063\n",
      "irradiated A549 cells (6 Gy γ-ray treatment)                         12216\n",
      "mixed sample treatments include DMSO, ARS-1620, and Vemurafenib      11968\n",
      "CAR T cell therapy, SUV39H1 knockout                                 10886\n",
      "infected with H1N1 (A/California/07/2009)                             9998\n",
      "control                                                               8649\n",
      "in vitro culture of iPSC-RPE                                          8117\n",
      "CAR T cell therapy with SUV39H1 knockout                              7693\n",
      "Infected with H1N1 (A/California/07/2009), 8 hours post infection     7123\n",
      "glyconanomaterials for combating bacterial infections                 6532\n",
      "Infected with H3N2 (A/Perth/16/2009), 16 hours post infection         5941\n",
      "lentiviral pool for expression of 120 gRNAs, tamoxifen, puromycin     5727\n",
      "Bexmarilimab                                                          5529\n",
      "Infected (Cal07, 16 hours, Replicate 2)                               4858\n",
      "infected with H3N2, Cetuximab (anti-EGFR therapy)                     4707\n",
      "8 hours post infection                                                4361\n",
      "infected with H3N2 (A/Perth/16/2009), 16 hours post infection         4021\n",
      "antineoplastic agents, mixed micelles for drug delivery               3629\n",
      "wild-type (WT) virus                                                  2803\n",
      "NS1 4xstop (mutant)                                                   2663\n",
      "unsure                                                                1259\n",
      "Ritonavir, gemcitabine, cisplatin                                     1096\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(combined.obs['perturbation'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AnnData object with n_obs × n_vars = 246262 × 33388\n",
       "    obs: 'gene_count', 'umi_count', 'SRX_accession', 'sample_id', 'batch', 'n_genes', 'perturbation'\n",
       "    var: 'gene_symbols', 'feature_types', 'n_cells'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file = \"a549_perturbation_analysis_loka/a549_combined_data_loka.h5ad\"\n",
    "combined.write_h5ad(output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = combined.X[0].toarray().flatten().tolist()  # 如果 .X 是 sparse matrix\n",
    "# a = [i for i in a if i > 0] \n",
    "# a, len(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Below is previous code, above all is the modified code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scanpy as sc\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "import anndata\n",
    "\n",
    "# Directory containing the downloaded A549 files\n",
    "data_dir = \"a549_data\"\n",
    "\n",
    "# List all h5ad files in the directory\n",
    "a549_files = [file for file in os.listdir(data_dir) if file.endswith('.h5ad')]\n",
    "print(f\"Found {len(a549_files)} A549 h5ad files\")\n",
    "\n",
    "# Create output directories\n",
    "results_dir = \"a549_perturbation_analysis\"\n",
    "os.makedirs(results_dir, exist_ok=True)\n",
    "fig_dir = os.path.join(results_dir, \"figures\")\n",
    "os.makedirs(fig_dir, exist_ok=True)\n",
    "\n",
    "# Load all datasets\n",
    "adatas = []\n",
    "for file in a549_files:\n",
    "    file_path = os.path.join(data_dir, file)\n",
    "    print(f\"Loading {file}...\")\n",
    "    try:\n",
    "        adata = sc.read_h5ad(file_path)\n",
    "        # Extract accession ID from filename\n",
    "        accession = file.replace('.h5ad', '')\n",
    "        adata.obs['sample_id'] = accession\n",
    "        adatas.append(adata)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {file}: {e}\")\n",
    "\n",
    "\n",
    "# Combine the data\n",
    "print(\"Combining data...\")\n",
    "combined = adatas[0].concatenate(adatas[1:], join='outer', index_unique='-')\n",
    "print(f\"Combined data shape: {combined.shape}\")\n",
    "\n",
    "# Basic preprocessing\n",
    "sc.pp.filter_cells(combined, min_genes=200)\n",
    "sc.pp.filter_genes(combined, min_cells=10)\n",
    "sc.pp.normalize_total(combined, target_sum=1e4)\n",
    "sc.pp.log1p(combined)\n",
    "\n",
    "import pyarrow.dataset as ds\n",
    "import gcsfs\n",
    "\n",
    "fs = gcsfs.GCSFileSystem()\n",
    "gcs_base_path = \"gs://arc-ctc-scbasecamp/2025-02-25/\"\n",
    "feature_type = \"GeneFull_Ex50pAS\"\n",
    "metadata_path = os.path.join(gcs_base_path, \"metadata\", feature_type)\n",
    "sample_metadata_path = os.path.join(metadata_path, \"Homo_sapiens\", \"sample_metadata.parquet\")\n",
    "sample_metadata = ds.dataset(sample_metadata_path, filesystem=fs, format=\"parquet\").to_table().to_pandas()\n",
    "\n",
    "# Filter for A549 cell line and extract perturbation information\n",
    "a549_metadata = sample_metadata[sample_metadata[\"cell_line\"].str.contains(\"A549\", na=False)]\n",
    "\n",
    "# Map sample_id to perturbation information\n",
    "id_to_perturbation = dict(zip(a549_metadata[\"srx_accession\"], a549_metadata[\"perturbation\"]))\n",
    "\n",
    "# Add perturbation information to the anndata object\n",
    "combined.obs['perturbation'] = combined.obs['sample_id'].map(id_to_perturbation).fillna('unknown')\n",
    "\n",
    "# Identify control samples (no perturbation) - look for keywords in perturbation column\n",
    "control_keywords = ['control', 'untreated', 'dmso', 'vehicle', 'none', 'mock']\n",
    "combined.obs['is_control'] = combined.obs['perturbation'].str.lower().apply(\n",
    "    lambda x: any(keyword in str(x).lower() for keyword in control_keywords) if pd.notna(x) else False\n",
    ")\n",
    "\n",
    "# If no explicit controls are found, try to identify them from the metadata context\n",
    "if combined.obs['is_control'].sum() == 0:\n",
    "    print(\"No explicit control samples found. Analyzing perturbation patterns...\")\n",
    "    \n",
    "    # Create a summary of perturbation conditions\n",
    "    perturbation_summary = combined.obs['perturbation'].value_counts()\n",
    "    print(perturbation_summary)\n",
    "    \n",
    "    # Ask user to specify control condition if automatic detection fails\n",
    "    print(\"Please manually review perturbation conditions and define controls.\")\n",
    "\n",
    "# Print summary of control vs treatment samples\n",
    "print(f\"Control samples: {combined.obs['is_control'].sum()}\")\n",
    "print(f\"Treatment samples: {(~combined.obs['is_control']).sum()}\")\n",
    "\n",
    "# Group perturbations by drug name where possible\n",
    "# This requires some text processing as perturbation descriptions vary\n",
    "def extract_drug_name(perturbation_text):\n",
    "    if pd.isna(perturbation_text):\n",
    "        return 'unknown'\n",
    "    \n",
    "    perturbation_text = str(perturbation_text).lower()\n",
    "    \n",
    "    # Skip control conditions\n",
    "    if any(keyword in perturbation_text for keyword in control_keywords):\n",
    "        return 'control'\n",
    "    \n",
    "    # Try to extract drug names - this would need refinement based on actual data\n",
    "    # Example logic - this should be adjusted based on your actual data format\n",
    "    if 'treated with' in perturbation_text:\n",
    "        parts = perturbation_text.split('treated with')\n",
    "        if len(parts) > 1:\n",
    "            drug_part = parts[1].strip()\n",
    "            # Take the first word as potential drug name\n",
    "            drug_name = drug_part.split()[0].strip(',.:;')\n",
    "            return drug_name\n",
    "    \n",
    "    # Add more extraction rules as needed\n",
    "    \n",
    "    # Default return the first 30 chars if no pattern matched\n",
    "    return perturbation_text[:30]\n",
    "\n",
    "combined.obs['drug'] = combined.obs['perturbation'].apply(extract_drug_name)\n",
    "\n",
    "# For each drug, perform hypothesis testing to identify differentially expressed genes\n",
    "control_cells = combined[combined.obs['is_control']]\n",
    "if len(control_cells) == 0:\n",
    "    print(\"Error: No control cells identified. Cannot perform differential expression analysis.\")\n",
    "    exit()\n",
    "\n",
    "# Create results dataframe to store findings\n",
    "results = pd.DataFrame()\n",
    "\n",
    "# Get unique drugs (excluding control)\n",
    "unique_drugs = combined.obs['drug'].unique()\n",
    "unique_drugs = [drug for drug in unique_drugs if drug != 'control']\n",
    "\n",
    "print(f\"Analyzing differential expression for {len(unique_drugs)} drugs...\")\n",
    "\n",
    "for drug in unique_drugs:\n",
    "    print(f\"Processing drug: {drug}\")\n",
    "    \n",
    "    # Get cells treated with this drug\n",
    "    drug_cells = combined[combined.obs['drug'] == drug]\n",
    "    \n",
    "    if len(drug_cells) < 10:\n",
    "        print(f\"  Skipping {drug}: too few cells ({len(drug_cells)})\")\n",
    "        continue\n",
    "    \n",
    "    # Perform differential expression analysis\n",
    "    try:\n",
    "        sc.tl.rank_genes_groups(combined, 'drug', groups=[drug], reference='control', method='wilcoxon')\n",
    "        \n",
    "        # Extract results for this drug\n",
    "        de_genes = sc.get.rank_genes_groups_df(combined, group=drug)\n",
    "        de_genes['drug'] = drug\n",
    "        \n",
    "        # Filter for significantly differentially expressed genes\n",
    "        significant_genes = de_genes[de_genes['pvals_adj'] < 0.05]\n",
    "        \n",
    "        # Add to results\n",
    "        results = pd.concat([results, significant_genes])\n",
    "        \n",
    "        # Create volcano plot for top genes\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        plt.scatter(de_genes['logfoldchanges'], -np.log10(de_genes['pvals']), alpha=0.5)\n",
    "        \n",
    "        # Highlight significant genes\n",
    "        significant = (de_genes['pvals_adj'] < 0.05)\n",
    "        plt.scatter(\n",
    "            de_genes.loc[significant, 'logfoldchanges'],\n",
    "            -np.log10(de_genes.loc[significant, 'pvals']),\n",
    "            color='red', alpha=0.8\n",
    "        )\n",
    "        \n",
    "        # Label top genes\n",
    "        top_genes = de_genes.nsmallest(10, 'pvals')\n",
    "        for _, gene in top_genes.iterrows():\n",
    "            plt.annotate(gene['names'], \n",
    "                        (gene['logfoldchanges'], -np.log10(gene['pvals'])),\n",
    "                        xytext=(5, 5), textcoords='offset points')\n",
    "        \n",
    "        plt.axhline(-np.log10(0.05), linestyle='--', color='gray')\n",
    "        plt.axvline(-1, linestyle='--', color='gray')\n",
    "        plt.axvline(1, linestyle='--', color='gray')\n",
    "        \n",
    "        plt.xlabel('Log Fold Change')\n",
    "        plt.ylabel('-log10(p-value)')\n",
    "        plt.title(f'Differential Expression: {drug} vs Control')\n",
    "        plt.savefig(os.path.join(fig_dir, f'volcano_plot_{drug}.png'))\n",
    "        plt.close()\n",
    "        \n",
    "        # Save top genes list for this drug\n",
    "        top_n = min(50, len(significant_genes))\n",
    "        significant_genes.head(top_n).to_csv(\n",
    "            os.path.join(results_dir, f'top_genes_{drug}.csv'), index=False)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  Error analyzing {drug}: {e}\")\n",
    "\n",
    "# Save combined results\n",
    "if not results.empty:\n",
    "    # Summary of all drugs and their significant genes\n",
    "    drug_gene_counts = results.groupby('drug').size().reset_index(name='sig_gene_count')\n",
    "    drug_gene_counts = drug_gene_counts.sort_values('sig_gene_count', ascending=False)\n",
    "    \n",
    "    # Save summary\n",
    "    drug_gene_counts.to_csv(os.path.join(results_dir, 'drug_affected_gene_counts.csv'), index=False)\n",
    "    \n",
    "    # Save full results\n",
    "    results.to_csv(os.path.join(results_dir, 'all_drug_gene_effects.csv'), index=False)\n",
    "    \n",
    "    # Create summary figure of drugs by number of affected genes\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.barplot(x='drug', y='sig_gene_count', data=drug_gene_counts)\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.title('Number of Significantly Affected Genes by Drug')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(fig_dir, 'drug_gene_count_summary.png'))\n",
    "    plt.close()\n",
    "    \n",
    "    # Identify most frequently affected genes across multiple drugs\n",
    "    gene_drug_counts = results.groupby('names').size().reset_index(name='drug_count')\n",
    "    gene_drug_counts = gene_drug_counts.sort_values('drug_count', ascending=False)\n",
    "    \n",
    "    # Save genes affected by multiple drugs\n",
    "    gene_drug_counts.head(100).to_csv(os.path.join(results_dir, 'multi_drug_affected_genes.csv'), index=False)\n",
    "    \n",
    "    # Create heatmap of top genes across drugs\n",
    "    top_genes = gene_drug_counts.head(20)['names'].tolist()\n",
    "    top_drugs = drug_gene_counts.head(15)['drug'].tolist()\n",
    "    \n",
    "    # Filter results for top genes and drugs\n",
    "    heatmap_data = results[\n",
    "        (results['names'].isin(top_genes)) & \n",
    "        (results['drug'].isin(top_drugs))\n",
    "    ]\n",
    "    \n",
    "    if not heatmap_data.empty:\n",
    "        # Create pivot table for heatmap\n",
    "        pivot_data = heatmap_data.pivot_table(\n",
    "            index='names', \n",
    "            columns='drug', \n",
    "            values='logfoldchanges',\n",
    "            fill_value=0\n",
    "        )\n",
    "        \n",
    "        # Create heatmap\n",
    "        plt.figure(figsize=(15, 10))\n",
    "        sns.heatmap(pivot_data, cmap='RdBu_r', center=0, annot=False)\n",
    "        plt.title('Log Fold Changes of Top Genes Across Drugs')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(fig_dir, 'gene_drug_heatmap.png'))\n",
    "        plt.close()\n",
    "\n",
    "print(f\"Analysis complete. Results saved to {results_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file = \"a549_combined_data.h5ad\"\n",
    "combined.write_h5ad(output_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 34 A549 h5ad files\n",
      "Processed metadata for SRX17915870.h5ad\n",
      "Processed metadata for SRX17150748.h5ad\n",
      "Processed metadata for SRX25289884.h5ad\n",
      "Processed metadata for SRX21897873.h5ad\n",
      "Processed metadata for SRX25289889.h5ad\n",
      "Processed metadata for SRX25289882.h5ad\n",
      "Processed metadata for SRX17150747.h5ad\n",
      "Processed metadata for SRX25289890.h5ad\n",
      "Processed metadata for ERX8792190.h5ad\n",
      "Processed metadata for SRX19215443.h5ad\n",
      "Processed metadata for SRX25289894.h5ad\n",
      "Processed metadata for SRX17915869.h5ad\n",
      "Processed metadata for SRX25289893.h5ad\n",
      "Processed metadata for SRX17488180.h5ad\n",
      "Processed metadata for SRX26771412.h5ad\n",
      "Processed metadata for SRX17150749.h5ad\n",
      "Processed metadata for SRX25289891.h5ad\n",
      "Processed metadata for SRX21897869.h5ad\n",
      "Processed metadata for SRX19215444.h5ad\n",
      "Processed metadata for SRX22159982.h5ad\n",
      "Processed metadata for SRX17150750.h5ad\n",
      "Processed metadata for SRX25289892.h5ad\n",
      "Processed metadata for SRX25289886.h5ad\n",
      "Processed metadata for SRX25289887.h5ad\n",
      "Processed metadata for SRX17941758.h5ad\n",
      "Processed metadata for SRX24227811.h5ad\n",
      "Processed metadata for SRX21897872.h5ad\n",
      "Processed metadata for SRX17941757.h5ad\n",
      "Processed metadata for SRX25289881.h5ad\n",
      "Processed metadata for SRX25289879.h5ad\n",
      "Processed metadata for SRX25289885.h5ad\n",
      "Processed metadata for SRX25289888.h5ad\n",
      "Processed metadata for SRX25289880.h5ad\n",
      "Processed metadata for SRX19004457.h5ad\n",
      "Saved metadata for 34 files\n",
      "Created documentation in a549_combined_data/README.txt\n"
     ]
    }
   ],
   "source": [
    "metadata = []\n",
    "output_dir = \"a549_combined_data\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "# List all h5ad files in the directory\n",
    "a549_files = [file for file in os.listdir(data_dir) if file.endswith('.h5ad')]\n",
    "print(f\"Found {len(a549_files)} A549 h5ad files\")\n",
    "\n",
    "# Save basic metadata for each file\n",
    "for file in a549_files:\n",
    "    file_path = os.path.join(data_dir, file)\n",
    "    try:\n",
    "        # Load basic info without reading full data\n",
    "        adata = sc.read_h5ad(file_path, backed='r')\n",
    "        \n",
    "        metadata.append({\n",
    "            'filename': file,\n",
    "            'filepath': file_path,\n",
    "            'n_obs': adata.n_obs,\n",
    "            'n_vars': adata.n_vars,\n",
    "            'obs_keys': list(adata.obs.keys()),\n",
    "            'var_keys': list(adata.var.keys()),\n",
    "        })\n",
    "        \n",
    "        print(f\"Processed metadata for {file}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {file}: {e}\")\n",
    "\n",
    "# Save metadata as CSV\n",
    "metadata_df = pd.DataFrame(metadata)\n",
    "metadata_df.to_csv(os.path.join(output_dir, \"a549_files_metadata.csv\"), index=False)\n",
    "print(f\"Saved metadata for {len(metadata_df)} files\")\n",
    "\n",
    "# Create a readme file with loading instructions\n",
    "with open(os.path.join(output_dir, \"README.txt\"), 'w') as f:\n",
    "    f.write(\"A549 Cell Line Data Collection\\n\")\n",
    "    f.write(\"=============================\\n\\n\")\n",
    "    f.write(f\"This directory contains metadata for {len(metadata_df)} A549 cell line h5ad files.\\n\\n\")\n",
    "    f.write(\"To recreate the combined dataset, use the following Python code:\\n\\n\")\n",
    "    f.write(\"```python\\n\")\n",
    "    f.write(\"import scanpy as sc\\n\")\n",
    "    f.write(\"import pandas as pd\\n\")\n",
    "    f.write(\"import os\\n\\n\")\n",
    "    f.write(\"# Load the metadata\\n\")\n",
    "    f.write(\"metadata = pd.read_csv('a549_files_metadata.csv')\\n\\n\")\n",
    "    f.write(\"# Load all datasets\\n\")\n",
    "    f.write(\"adatas = []\\n\")\n",
    "    f.write(\"for file_path in metadata['filepath']:\\n\")\n",
    "    f.write(\"    adata = sc.read_h5ad(file_path)\\n\")\n",
    "    f.write(\"    # Add sample ID from filename\\n\")\n",
    "    f.write(\"    adata.obs['sample_id'] = os.path.basename(file_path).replace('.h5ad', '')\\n\")\n",
    "    f.write(\"    adatas.append(adata)\\n\\n\")\n",
    "    f.write(\"# Combine datasets\\n\")\n",
    "    f.write(\"combined = adatas[0].concatenate(adatas[1:], join='outer', index_unique='-')\\n\")\n",
    "    f.write(\"print(f'Combined data shape: {combined.shape}')\\n\")\n",
    "    f.write(\"```\\n\")\n",
    "\n",
    "print(f\"Created documentation in {output_dir}/README.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llk",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
